{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f820df82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=$PATH:/opt/X11/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x11b5043d0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env PATH=$PATH:/opt/X11/bin\n",
    "from pyvirtualdisplay.display import Display\n",
    "\n",
    "virtual_display = Display(visible=False, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dd6a1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fe223c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Discrete(64)\n",
      "sample observation: 3\n",
      "action space shape: Discrete(4)\n",
      "action space sample: 3\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', map_name='8x8',\n",
    "               is_slippery=False, render_mode='rgb_array')\n",
    "print(f'observation space: {env.observation_space}')\n",
    "print(f'sample observation: {env.observation_space.sample()}')\n",
    "print(f'action space shape: {env.action_space}')\n",
    "print(f'action space sample: {env.action_space.sample()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "93f4918e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Q-table.\n",
    "def initialize_q_table(state_space, action_space):\n",
    "    return np.zeros((state_space, action_space))\n",
    "\n",
    "\n",
    "q_table = initialize_q_table(env.observation_space.n, env.action_space.n)\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cfa88a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define policies.\n",
    "def argmax(arr):\n",
    "    arr_max = np.max(arr)\n",
    "    return np.random.choice(np.where(arr == arr_max)[0])\n",
    "\n",
    "\n",
    "def greedy_policy(q_table, state):\n",
    "    return argmax(q_table[state][:])\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(env: gym.Env, q_table: np.ndarray, state: int,\n",
    "                          epsilon: float):\n",
    "    random_num = random.uniform(0, 1)\n",
    "    # Exploit.\n",
    "    if random_num > epsilon:\n",
    "        return greedy_policy(q_table, state)\n",
    "    # Explore.\n",
    "    else:\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "05022848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training params.\n",
    "n_training_episodes = 1_000_000\n",
    "learning_rate = 0.1\n",
    "n_eval_episodes = 100\n",
    "env_id = 'FrozenLake-v1'\n",
    "max_steps = 99\n",
    "gamma = 0.95\n",
    "eval_seed = []\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.001\n",
    "decay_rate = (2 * max_epsilon) / n_training_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "10bfe810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env: gym.Env, q_table: np.ndarray, n_training_episodes: int,\n",
    "          max_steps: int, learning_rate: float, min_epsilon: float,\n",
    "          max_epsilon: float, decay_rate: float, gamma: float):\n",
    "    _q_table = q_table.copy()\n",
    "    for episode in tqdm(range(n_training_episodes)):\n",
    "        # Reduce epsilon progressively.\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * \\\n",
    "            np.exp(-decay_rate * episode)\n",
    "        state, _ = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        for _ in range(max_steps):\n",
    "            # Choose action At using epsilon greedy policy.\n",
    "            action = epsilon_greedy_policy(env, _q_table, state, epsilon)\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            _q_table[state][action] += learning_rate * \\\n",
    "                (float(reward) + gamma *\n",
    "                 np.max(_q_table[state]) - _q_table[state][action])\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = new_state\n",
    "    return _q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b32d279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [05:26<00:00, 3064.02it/s]\n"
     ]
    }
   ],
   "source": [
    "trained_q_table = train(env, q_table, n_training_episodes, max_steps,\n",
    "                        learning_rate, min_epsilon, max_epsilon, decay_rate,\n",
    "                        gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8d0e23be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [18.99999988, 19.99999999, 18.99999974, 18.99999992],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 5.98284381,  6.72868409, 12.12731247,  7.45443757],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a5927e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, q_table, seed):\n",
    "    episode_rewards = []\n",
    "    for episode in tqdm(range(n_eval_episodes)):\n",
    "        state, info = env.reset(seed=seed[episode] if seed else None)\n",
    "        step = 0\n",
    "        truncated = False\n",
    "        terminated = False\n",
    "        total_reward_ep = 0\n",
    "        for step in range(max_steps):\n",
    "            action = greedy_policy(q_table, state)\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward_ep += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_reward_ep)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0c976593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2025.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=0.0; std=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes,\n",
    "                                         trained_q_table, eval_seed)\n",
    "print(f'mean_reward={mean_reward}; std={std_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7bc43e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_video(env: gym.Env, q_table: np.ndarray, out_dir, fps=1):\n",
    "    images = []\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    state, _ = env.reset(seed=random.randint(0, 500))\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "    while True:\n",
    "        action = np.argmax(q_table[state][:])\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        img = env.render()\n",
    "        images.append(img)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    imageio.mimsave(out_dir, [np.array(img) for img in images], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3d5443b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_video(env, trained_q_table, './out.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
